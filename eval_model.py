import argparse
import torch
from transformers import AutoTokenizer
from self_link_model import SelfLinkLM, SelfLinkConfig

def load_model(model_path, tokenizer_path):
    """åŠ è½½è‡ªé“¾æ¥æ¨¡å‹å’Œtokenizer"""
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    
    # åŠ è½½æ¨¡å‹é…ç½®
    config = SelfLinkConfig(
        vocab_size=tokenizer.vocab_size,
        n_embd=768,
        n_layer=12,
        n_head=12,
        self_link_ratio=0.1
    )
    
    # åˆå§‹åŒ–å¹¶åŠ è½½æ¨¡å‹
    model = SelfLinkLM(config)
    state_dict = torch.load(model_path, map_location='cpu', weights_only=False)
    model.load_state_dict(state_dict['model_state_dict'])
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    return model.to(device).eval(), tokenizer

def interactive_test(model, tokenizer):
    """äº¤äº’å¼æµ‹è¯•æ¨¡å‹"""
    print("è‡ªé“¾æ¥æ¨¡å‹äº¤äº’æµ‹è¯• (è¾“å…¥'quit'é€€å‡º)")
    while True:
        prompt = input("\nğŸ‘¤ è¯·è¾“å…¥: ")
        if prompt.lower() == 'quit':
            break
            
        # ç¼–ç è¾“å…¥
        inputs = tokenizer(prompt, return_tensors='pt').to(model.device)
        
        # æ‰‹åŠ¨å®ç°æ–‡æœ¬ç”Ÿæˆ
        print("ğŸ¤– æ¨¡å‹å›å¤: ", end='', flush=True)
        generated = inputs['input_ids'].clone()
        max_length = inputs['input_ids'].shape[1] + 200
        
        with torch.no_grad():
            while generated.shape[1] < max_length:
                outputs = model(
                    input_ids=generated,
                    attention_mask=torch.ones_like(generated)
                )
                
                # è·å–æœ€åä¸€ä¸ªtokençš„logits
                next_token_logits = outputs['logits'][:, -1, :]
                
                # æ¸©åº¦é‡‡æ ·
                next_token_logits = next_token_logits / 0.9
                probs = torch.softmax(next_token_logits, dim=-1)
                
                # top-pé‡‡æ ·
                sorted_probs, sorted_indices = torch.sort(probs, descending=True)
                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                sorted_indices_to_remove = cumulative_probs > 0.9
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                probs[:, indices_to_remove] = 0
                
                # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                next_token = torch.multinomial(probs, num_samples=1)
                generated = torch.cat([generated, next_token], dim=-1)
                
                # æ‰“å°æ–°ç”Ÿæˆçš„token
                new_text = tokenizer.decode(next_token[0], skip_special_tokens=True)
                print(new_text, end='', flush=True)
                
                # é‡åˆ°ç»“æŸç¬¦åˆ™åœæ­¢
                if next_token.item() == tokenizer.eos_token_id:
                    break
                    
        print()  # æ¢è¡Œ

def main():
    parser = argparse.ArgumentParser(description='è‡ªé“¾æ¥æ¨¡å‹è¯„ä¼°è„šæœ¬')
    parser.add_argument('--model_path', default='output_pretrain/model.pth',
                      help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--tokenizer_path', default='output_pretrain',
                      help='tokenizerè·¯å¾„')
    args = parser.parse_args()

    model, tokenizer = load_model(args.model_path, args.tokenizer_path)
    print(f"æ¨¡å‹å·²åŠ è½½ï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
    
    interactive_test(model, tokenizer)

if __name__ == "__main__":
    main()